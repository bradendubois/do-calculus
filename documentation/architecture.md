# Architecture

An outline of the design and architecture of the software.

#### Main

This is the file to be run directly. Its order of operations are as follows:

- If enabled, a shell script is run which will run a ``git pull``; this will (hopefully) allow the software to update without a restart.
- If enabled, the regression suite first runs. Results are returned to Main, which may be output.
- After this, the graph file location specified is read, and listed. The user selects a file, which is loaded into the REPL Driver.
- The user can interact with the main REPL Driver until exiting, after which they are prompted to select another file or exit.

Below is each subdirectory of the project, with a description of its purpose and each file within it.

## Causal Graphs

This is the default subdirectory containing each Causal Graph file, which can be loaded by the software.

For more information on Causal Graph files, see ``documentation/Causal Graph Files``.

## Config

This subdirectory controls all configuration file settings, from creating a configuration file to accessing them, repairing the user's file, etc.

- ``config.json``: Generated on running the software for the first time: a basic JSON file of all settings.
- ``config_manager.py``: Can be directly run, and is imported into the software; an ``access`` method is provided as a way of abstracting accessing specific settings. This option allows the software to detect invalid or missing settings and revert it to a pre-defined "default".
- ``primary_configuration.py``: A primary/source file that defines all settings for the project: it's broken into sections, with every parameter having its settings name, a documentation name, a default value, a set of options, etc.
- ``generate_config_docs.py``: By using the ``primary_configuration.py``, a Markdown file is generated and placed in ``documentation``; all the settings and details are outlined in this file, named ``Configuration``.

## Debug

This subdirectory is a collection of scripts and text files used for various debugging / development purposes. It's the scrap paper of development, and anything in here is not likely to work. It's mostly used to test something or flesh out an idea.

**Nothing** in the "main" software will ever use or rely on anything located in ``debug``. As well, a "build" may not even have this directory present.

## Documentation

This subdirectory contains all the documentation for the project (with the exception of the README and Quick Usage in the root of the project).

The subdirectory ``readme_sections`` contains 3 files, README_1, README_2, README_3. These files are "stitched together" to create the README placed in the root of the project.

- ``README_1``: An introduction / requirements list for running the project.
- ``README_2``: A "Quick Usage", which is actually turned into the Quick Usage at the root of the project.
- ``README_3``: A "details" section which outlines all the files in ``documentation`` to seek for further information.

Quick Usage is actually the middle section of README; this is simply done for, aptly-named, Quick Usage.

**Note**: ``readme_sections`` may not be present in a "build", though the compiled Quick Usage / README will be.

- ``Architecture``: This file! Outlining the purpose/layout of each component of the project.
- ``Causal Graph Files``: Outlines the layout and creation of Causal Graph files, for use in the project.
- ``Configuration``: The documentation on configuration files, as generated by ``config/generate_config_docs.py``.
- ``Decorator Usage``: Not incredibly relevant, but Python decorators are implemented to allow functions to be timed/debugged. A short explanation of how they are used.
- ``Regression Tests``: Outlines how the regression suite works and how to create test files.

## Logs

This subdirectory is the default location for computations / regression tests to be logged to.

- ``regression`` is the default subdirectory to which regression tests will be entirely written. A "lock" is placed on the IO logging module so that the entire regression suite will be logged to one file, which is named after the date/time the suite is started.

## Probability Structures

This is the "main" section of the software. Main functionality is broken up amongst the REPL Driver, the Causal Graph, the BackdoorController, and the Probability Engine; this is to reduce the dependence of "god classes", keep every relatively modular, etc. Functionality regarding traversal of the actual graph file is unified through all these through the use of the Graph class.

- ``BackdoorController.py``: This has its own mini-REPL system that, when run, will ask the user for two sets of variables and find any sufficient de-confounding sets Z. It is also used through the *do*-calculus of Pearl with various modified path-finding algorithms implemented.
- ``Causal_Graph.py``: Handles most work other than the *do*-calculus itself. It can take in input for probability queries and find sufficient Z sets using the BackdoorController, modifying the given query accordingly. Also responsible for generating the joint distribution tables and seeing topological orderings of the graphs.
- ``ConditionalProbabilityTable.py``: Each table is used to hold the data for each respective variable; it stores the variable it is "for", and rows of each combination of outcome and parent-outcomes, with a respective probability.
- ``Graph.py``: A (relatively) standard graph, with modifications to support the graph manipulation of *do*-calculus. Supports easy methods for ancestors/reach, as well "disabling" incoming/outgoing edges, which aligns with the graph manipulation of *do*-calculus.
- ``Probability_Engine.py``: The isolated code that takes a given query, such as P(Y=y | X=x) in the form of Outcomes/Interventions, and can actually apply all the standard inference rules to compute it, relying on the Graph for traversal/relations, as well as the Conditional Probability Tables for actual values.
- ``REPL_Driver.py``: This will take a parsed graph file, unpack and store its contents, and will begin to drive the main IO of the software until the user chooses to exit / switch files. It handles a bit of data input / pre-processing when possible, but usually simply calls a relevant method from a Causal Graph instance.
- ``VariableStructures.py``: Holds 3 different structures regarding Variables, as well as a parser than breaks a string of values into respective Outcomes and Interventions.
  - ``Variable``: A "variable" in the Causal Graph, which has a unique name, some list of discrete outcomes, a list of parent Variables.
  - ``Outcome``: Represents a specific instance of a Variable, where some specific outcome from the set of possible outcomes is given.
  - ``Intervention``: Identical to the Outcome, but differentiated for purposes of *do*-calculus in being a treatment rather than observed outcome.

The subdirectory ``do_calculus`` relies on pieces of the above, but is rather isolated and serves the purpose of taking some initial query from the user involving *do*'s/treatments, and applying the 3 rules of *do*-calculus along with standard inference rules to arrive at a *do*-less/"hat"-less equivalent query.

The subdirectory ``ids_ai`` implements a basic IDS AI solver which can take some initial data and attempt to apply all the rules of *do*-calculus and standard inference to derive a hat-less equivalent expression.

### do_calculus

- ``DoCalculus.py``: A REPL system that will take in all the initial variables, specify unobservable variables, etc. and allow the user to either apply the rules on their own, step-by-step, or allow an IDS AI to attempt to find a solution.

The subdirectory ``application`` more specifically handles the actual rules, derivations of Queries, etc.

#### application

- ``CustomSetFunctions.py``: Since variable are re-named/differentiated in the query manipulating, such as X vs. X', standard set functions do not work. Custom subtraction, union, renaming, un-renaming methods are provided.
- ``DoCalculusQueryOptions``: Takes some QueryList object and the corresponding graph, and will derive all possible steps that can be applied to the Query.
- ``QueryListParser.py``: Takes some QueryList and set of "known" data for the initial/non-renamed variables and compute a probability.
- ``QueryStructures.py``: Contains a few classes that neatly abstract the process of query manipulation and rule application.
  - ``Sigma``: A basic Sigma symbol, taking some set of variables that the subsequent section of the QueryList should be summed over.
  - ``QueryBody``: Two sets of variables, the *interventions*/treatments, and observations.
  - ``Query``: An actual query representing P(y | do(x), observe(w)).
  - ``QueryList``: Some query that takes an arbitrary number of Sigma's and Query's (which would be considered "sub-queries").

A subdirectory ``rules`` is included, and stores the *do*-calculus rules as well as a couple standard inference rules.

- ``DoCalculusRules.py``: Stores rules 1-3 of the *do*-calculus, where each rule has 2 functions; one that verifies the rule applies given some data, and another that returns the resulting Sigma/Query objects.
- ``StandardInferenceRules.py``: Stores two functions that can apply the product rule and condition over some variable.

### ids_ai

- ``IDS_Solver.py``: A pretty standard iterative-deepening searcher looking for a result/answer.
- ``Solution.py``: Holds a basic ``Solution`` class to return the data / steps taken from a search.
- ``Stack.py``: A completely standard Stack used in the IDS Solver.

## Scripts

This subdirectory contains a few shell scripts.

- ``build_readme.sh``: This is used to generate all documentation for the project, in which different Markdown files are (sometimes concatenated) and compiled to PDF, using pandoc.
- ``git_update.sh``: Very small; just does a "git pull" and sets some configurations so that Github will save the username/password so that future pulls will not need to login.
- ``prepare_build.sh``: Generates a "production build" of sorts; uploading the development directory as-is typically results in exceptionally large wait times, as subdirectories such as ``.git`` or ``.idea`` contain hundreds of small files. This will create a version-labelled subdirectory, copying all the Python code, causal graphs, regression suite, etc. All Markdown documentation will be compiled into PDF form. No logs will be copied, nor will the entire ``debug`` subdirectory``. This final build produced is ideal for sharing/sending to someone else to use if they will not be changing the code.

## Tests

This subdirectory contains the default location for test files, as well as a subdirectory for test-specific graph files.

For more information on how to create and add tests, see ``Regression Tests``.

- ``test_files``: The default subdirectory containing all the test files for the regression suite to run.
- ``test_graphs``: The default subdirectory to place a test-specific graph, which are not to be shown/loaded by the software.

## Util

A large collection of utilities / modules that are re-used throughout the project and as such, can be isolated from the main probability code module.

- ``decorators.py``: A couple of decorators (see ``Decorators``) that allow debugging and timing of functions.
- ``IO_Logger.py``: A main IO module that allows minimal coding to relay information to both the terminal output as well as being simultaneously written to a text file. Methods exist to open a file to write to, close any open file, lock the file stream to prevent switching files. Optional ``x_offset`` parameter in writing allows a multi-line message to be horizontally shifted to signal recursive depth; especially useful in probability queries.
- ``ProbabilityExceptions.py``: A collection of custom Exceptions to be raised when input is invalid, queries cannot be computed, etc. and caught, but are separate from standard AssertionErrors, NoneTypeErrors, etc. and are handled separately.
- ``RegressionTesting.py``: The main driver of the regression test suite (see ``Regression Tests``), in which each test file (by default, located in ``tests/test_files``) is loaded and run. Results across each file are stored and returned as a list of results, so that each test file can be viewed separately.
- ``ResultCache.py``: A simple abstraction of a dictionary, in which any string representation of a query can be stored with a value that is computed, so that it can be later looked up rather than recomputed. This can be enabled/disabled by the configuration file. **Note**: The dictionary is cleared on any file being loaded, as a variable in one file with the same name as a different variable will likely have cached different values.

The subdirectory ``helpers`` consists of a few very small "helpers", where their isolated code allows the "main" code to be far more readable.

- ``CallableItemWrapper.py``: A small menu system is created in ``parsers/UserIndexSelection``, and in generating/spacing in the menu, if the first component of an entry is "callable" (callable(print) -> True, callable(1) -> False), this component will not be shown. It provides a workaround to take a list of lists of data and strings, and still generate a menu.
- ``DisjointSets.py``: A tiny helper that takes an arbitrary number of sets and returns whether they are all completely disjoint.
- ``MinimizeSets.py``: Takes a set of sets and sorts them by size and returns a list of minimal sets to sufficiently represent the input sets. Used in the Backdoor Controller if the option to only show minimal sufficient Z sets is enabled.
- ``PowerSet.py``: Takes an iterable and returns a chain representing the iterable's power set. Optional flag to control whether to consider the empty set.

The subdirectory ``parsers`` contains modules used in parsing/reading input from the user for various purposes.

- ``GetHeadAndBody.py``: Used to abstract the process of asking the user for valid input on getting a valid head and body used in probability queries, such as P(Y=y | X=~x, Z=z).
- ``GraphLoader.py``: Used to complete load, read, and parse the contents of a Graph File (see ``Causal Graph Files``).
- ``ProbabilityString.py``: Used to take a head and body of a query, and convert it to a string representation, like "P(Y=y | do(X=x), Z=z)".
- ``UserIndexSelection.py``: Used to take a list, or list of lists, of data, and generate a well-formatted menu to the user and get an integer selection as to which index of the data they want to select.
